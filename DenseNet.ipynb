{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSxbrG36OWqHDB1fcJizmZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf1e1e94e8d24f67b1c575f385807daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7d16064f562740d4ba9038eadb2e7be4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ba7db11c30514a25b60fae45d92dacb6",
              "IPY_MODEL_3b744ece9d9e4d50ac69ca2e725a2c17"
            ]
          }
        },
        "7d16064f562740d4ba9038eadb2e7be4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba7db11c30514a25b60fae45d92dacb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9cdf334e8b684beca54d1cd6247f39fc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_64282ca47dd04bebba0d1726d0384e94"
          }
        },
        "3b744ece9d9e4d50ac69ca2e725a2c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6a2cf6538a10439fa452700e65384077",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 99204597.54it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38b550cffbec4232943597c6ccdfafcb"
          }
        },
        "9cdf334e8b684beca54d1cd6247f39fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "64282ca47dd04bebba0d1726d0384e94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a2cf6538a10439fa452700e65384077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38b550cffbec4232943597c6ccdfafcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EggPudding/Deep-Learning-Practice-with-Codes/blob/main/DenseNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDWUnEk4bMoA"
      },
      "source": [
        "## **Densely Connected Convolutional Networks (CVPR 2017) Tutorial**\r\n",
        "\r\n",
        "*   Pratice for DenseNet Architecture\r\n",
        "*   Orginal Paper: https://arxiv.org/abs/1608.06993\r\n",
        "*   Note that you first change **Runtime** to **GPU** setting\r\n",
        "*   **CIFAR-10** Dataset is used for practice for simplicity\r\n",
        "*   Part of codes from https://github.com/kuangliu/pytorch-cifar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnPgo_a7_EMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca5674a-a617-4b2c-9508-c810231a7994"
      },
      "source": [
        "!nvidia-smi # Make Sure you are using GPU"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jan 26 02:20:09 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zio-qsFKHb9K"
      },
      "source": [
        "### **DenseNet Model Definition**\r\n",
        "* In this **Tutorial**, **DenseNet-121** architecture is utilized since relatively simple dataset is used, **CIFAR-10**.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H7tlhaRHcEu"
      },
      "source": [
        "import math\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "import torch.backends.cudnn as cudnn\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "import os\r\n",
        "\r\n",
        "# Bottleneck Module\r\n",
        "# Module output is going to be concatenated with the model input,\r\n",
        "# this process is done recursively to construct DenseNet architecture.\r\n",
        "class Bottleneck(nn.Module):\r\n",
        "    def __init__(self, in_planes, growth_rate):\r\n",
        "        super(Bottleneck, self).__init__()\r\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\r\n",
        "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\r\n",
        "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\r\n",
        "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\r\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\r\n",
        "        out = torch.cat([out,x], 1) # Note that DenseNet utilized concatenation rather than skip connection as in ResNet\r\n",
        "        return out\r\n",
        "\r\n",
        "# Transition Module\r\n",
        "# Lower the resolution of activation map as well as reduce the channels of it.\r\n",
        "class Transition(nn.Module):\r\n",
        "    def __init__(self, in_planes, out_planes):\r\n",
        "        super(Transition, self).__init__()\r\n",
        "        self.bn = nn.BatchNorm2d(in_planes)\r\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.conv(F.relu(self.bn(x)))\r\n",
        "        out = F.avg_pool2d(out, 2)\r\n",
        "        return out\r\n",
        "\r\n",
        "# DenseNet Implementation\r\n",
        "class DenseNet(nn.Module):\r\n",
        "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\r\n",
        "        super(DenseNet, self).__init__()\r\n",
        "        self.growth_rate = growth_rate\r\n",
        "\r\n",
        "        num_planes = 2*growth_rate\r\n",
        "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\r\n",
        "\r\n",
        "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\r\n",
        "        num_planes += nblocks[0]*growth_rate\r\n",
        "        out_planes = int(math.floor(num_planes*reduction))\r\n",
        "        self.trans1 = Transition(num_planes, out_planes)\r\n",
        "        num_planes = out_planes\r\n",
        "\r\n",
        "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\r\n",
        "        num_planes += nblocks[1]*growth_rate\r\n",
        "        out_planes = int(math.floor(num_planes*reduction))\r\n",
        "        self.trans2 = Transition(num_planes, out_planes)\r\n",
        "        num_planes = out_planes\r\n",
        "\r\n",
        "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\r\n",
        "        num_planes += nblocks[2]*growth_rate\r\n",
        "        out_planes = int(math.floor(num_planes*reduction))\r\n",
        "        self.trans3 = Transition(num_planes, out_planes)\r\n",
        "        num_planes = out_planes\r\n",
        "\r\n",
        "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\r\n",
        "        num_planes += nblocks[3]*growth_rate\r\n",
        "\r\n",
        "        self.bn = nn.BatchNorm2d(num_planes)\r\n",
        "        self.linear = nn.Linear(num_planes, num_classes)\r\n",
        "\r\n",
        "    def _make_dense_layers(self, block, in_planes, nblock):\r\n",
        "        layers = []\r\n",
        "        for i in range(nblock):\r\n",
        "            layers.append(block(in_planes, self.growth_rate))\r\n",
        "            in_planes += self.growth_rate\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.trans1(self.dense1(out))\r\n",
        "        out = self.trans2(self.dense2(out))\r\n",
        "        out = self.trans3(self.dense3(out))\r\n",
        "        out = self.dense4(out)\r\n",
        "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\r\n",
        "        out = out.view(out.size(0), -1)\r\n",
        "        out = self.linear(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "def DenseNet121():\r\n",
        "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2p5PaQ-8B3v"
      },
      "source": [
        "### **Hyper Parameter Setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMnPH0YG8FuW"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # whether using gpu or cpu\r\n",
        "\r\n",
        "model = DenseNet121() # model assignment\r\n",
        "model.to(device) # mapping model weight & bias into gpu memory\r\n",
        "model = torch.nn.DataParallel(model) # used for parallel setting\r\n",
        "\r\n",
        "cudnn.benchmark = True # using cudnn which optimizes the algorithm\r\n",
        "\r\n",
        "learning_rate = 0.01\r\n",
        "batch_size = 128\r\n",
        "max_epoch = 10\r\n",
        "\r\n",
        "model_path = 'densenet121_cifar10.pt'\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss() # simple cross-entropy is utilized\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimizer utilized"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOh2w_xVc1cE"
      },
      "source": [
        "* **torchsummary** is package for visualizing pytorch model.\r\n",
        "* Layers, Number of parameters can be viewed through this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AefbsD-30AJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b894cdf-06ab-4db0-d6c4-500fbf5b6e24"
      },
      "source": [
        "import torchsummary\r\n",
        "\r\n",
        "torchsummary.summary(model.cuda(), (3, 32, 32)) # CIFAR-10 Image has 32 x 32 x 3 shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "            Conv2d-3          [-1, 128, 32, 32]           8,192\n",
            "       BatchNorm2d-4          [-1, 128, 32, 32]             256\n",
            "            Conv2d-5           [-1, 32, 32, 32]          36,864\n",
            "        Bottleneck-6           [-1, 96, 32, 32]               0\n",
            "       BatchNorm2d-7           [-1, 96, 32, 32]             192\n",
            "            Conv2d-8          [-1, 128, 32, 32]          12,288\n",
            "       BatchNorm2d-9          [-1, 128, 32, 32]             256\n",
            "           Conv2d-10           [-1, 32, 32, 32]          36,864\n",
            "       Bottleneck-11          [-1, 128, 32, 32]               0\n",
            "      BatchNorm2d-12          [-1, 128, 32, 32]             256\n",
            "           Conv2d-13          [-1, 128, 32, 32]          16,384\n",
            "      BatchNorm2d-14          [-1, 128, 32, 32]             256\n",
            "           Conv2d-15           [-1, 32, 32, 32]          36,864\n",
            "       Bottleneck-16          [-1, 160, 32, 32]               0\n",
            "      BatchNorm2d-17          [-1, 160, 32, 32]             320\n",
            "           Conv2d-18          [-1, 128, 32, 32]          20,480\n",
            "      BatchNorm2d-19          [-1, 128, 32, 32]             256\n",
            "           Conv2d-20           [-1, 32, 32, 32]          36,864\n",
            "       Bottleneck-21          [-1, 192, 32, 32]               0\n",
            "      BatchNorm2d-22          [-1, 192, 32, 32]             384\n",
            "           Conv2d-23          [-1, 128, 32, 32]          24,576\n",
            "      BatchNorm2d-24          [-1, 128, 32, 32]             256\n",
            "           Conv2d-25           [-1, 32, 32, 32]          36,864\n",
            "       Bottleneck-26          [-1, 224, 32, 32]               0\n",
            "      BatchNorm2d-27          [-1, 224, 32, 32]             448\n",
            "           Conv2d-28          [-1, 128, 32, 32]          28,672\n",
            "      BatchNorm2d-29          [-1, 128, 32, 32]             256\n",
            "           Conv2d-30           [-1, 32, 32, 32]          36,864\n",
            "       Bottleneck-31          [-1, 256, 32, 32]               0\n",
            "      BatchNorm2d-32          [-1, 256, 32, 32]             512\n",
            "           Conv2d-33          [-1, 128, 32, 32]          32,768\n",
            "       Transition-34          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-35          [-1, 128, 16, 16]             256\n",
            "           Conv2d-36          [-1, 128, 16, 16]          16,384\n",
            "      BatchNorm2d-37          [-1, 128, 16, 16]             256\n",
            "           Conv2d-38           [-1, 32, 16, 16]          36,864\n",
            "       Bottleneck-39          [-1, 160, 16, 16]               0\n",
            "      BatchNorm2d-40          [-1, 160, 16, 16]             320\n",
            "           Conv2d-41          [-1, 128, 16, 16]          20,480\n",
            "      BatchNorm2d-42          [-1, 128, 16, 16]             256\n",
            "           Conv2d-43           [-1, 32, 16, 16]          36,864\n",
            "       Bottleneck-44          [-1, 192, 16, 16]               0\n",
            "      BatchNorm2d-45          [-1, 192, 16, 16]             384\n",
            "           Conv2d-46          [-1, 128, 16, 16]          24,576\n",
            "      BatchNorm2d-47          [-1, 128, 16, 16]             256\n",
            "           Conv2d-48           [-1, 32, 16, 16]          36,864\n",
            "       Bottleneck-49          [-1, 224, 16, 16]               0\n",
            "      BatchNorm2d-50          [-1, 224, 16, 16]             448\n",
            "           Conv2d-51          [-1, 128, 16, 16]          28,672\n",
            "      BatchNorm2d-52          [-1, 128, 16, 16]             256\n",
            "           Conv2d-53           [-1, 32, 16, 16]          36,864\n",
            "       Bottleneck-54          [-1, 256, 16, 16]               0\n",
            "      BatchNorm2d-55          [-1, 256, 16, 16]             512\n",
            "           Conv2d-56          [-1, 128, 16, 16]          32,768\n",
            "      BatchNorm2d-57          [-1, 128, 16, 16]             256\n",
            "           Conv2d-58           [-1, 32, 16, 16]          36,864\n",
            "       Bottleneck-59          [-1, 288, 16, 16]               0\n",
            "      BatchNorm2d-60          [-1, 288, 16, 16]             576\n",
            "           Conv2d-61          [-1, 128, 16, 16]          36,864\n",
            "      BatchNorm2d-62          [-1, 128, 16, 16]             256\n",
            "           Conv2d-63           [-1, 32, 16, 16]          36,864\n",
            "       Bottleneck-64          [-1, 320, 16, 16]               0\n",
            "      BatchNorm2d-65          [-1, 320, 16, 16]             640\n",
            "           Conv2d-66          [-1, 128, 16, 16]          40,960\n",
            "      BatchNorm2d-67          [-1, 128, 16, 16]             256\n",
            "           Conv2d-68           [-1, 32, 16, 16]          36,864\n",
            "       Bottleneck-69          [-1, 352, 16, 16]               0\n",
            "      BatchNorm2d-70          [-1, 352, 16, 16]             704\n",
            "           Conv2d-71          [-1, 128, 16, 16]          45,056\n",
            "      BatchNorm2d-72          [-1, 128, 16, 16]             256\n",
            "           Conv2d-73           [-1, 32, 16, 16]          36,864\n",
            "       Bottleneck-74          [-1, 384, 16, 16]               0\n",
            "      BatchNorm2d-75          [-1, 384, 16, 16]             768\n",
            "           Conv2d-76          [-1, 128, 16, 16]          49,152\n",
            "      BatchNorm2d-77          [-1, 128, 16, 16]             256\n",
            "           Conv2d-78           [-1, 32, 16, 16]          36,864\n",
            "       Bottleneck-79          [-1, 416, 16, 16]               0\n",
            "      BatchNorm2d-80          [-1, 416, 16, 16]             832\n",
            "           Conv2d-81          [-1, 128, 16, 16]          53,248\n",
            "      BatchNorm2d-82          [-1, 128, 16, 16]             256\n",
            "           Conv2d-83           [-1, 32, 16, 16]          36,864\n",
            "       Bottleneck-84          [-1, 448, 16, 16]               0\n",
            "      BatchNorm2d-85          [-1, 448, 16, 16]             896\n",
            "           Conv2d-86          [-1, 128, 16, 16]          57,344\n",
            "      BatchNorm2d-87          [-1, 128, 16, 16]             256\n",
            "           Conv2d-88           [-1, 32, 16, 16]          36,864\n",
            "       Bottleneck-89          [-1, 480, 16, 16]               0\n",
            "      BatchNorm2d-90          [-1, 480, 16, 16]             960\n",
            "           Conv2d-91          [-1, 128, 16, 16]          61,440\n",
            "      BatchNorm2d-92          [-1, 128, 16, 16]             256\n",
            "           Conv2d-93           [-1, 32, 16, 16]          36,864\n",
            "       Bottleneck-94          [-1, 512, 16, 16]               0\n",
            "      BatchNorm2d-95          [-1, 512, 16, 16]           1,024\n",
            "           Conv2d-96          [-1, 256, 16, 16]         131,072\n",
            "       Transition-97            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-98            [-1, 256, 8, 8]             512\n",
            "           Conv2d-99            [-1, 128, 8, 8]          32,768\n",
            "     BatchNorm2d-100            [-1, 128, 8, 8]             256\n",
            "          Conv2d-101             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-102            [-1, 288, 8, 8]               0\n",
            "     BatchNorm2d-103            [-1, 288, 8, 8]             576\n",
            "          Conv2d-104            [-1, 128, 8, 8]          36,864\n",
            "     BatchNorm2d-105            [-1, 128, 8, 8]             256\n",
            "          Conv2d-106             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-107            [-1, 320, 8, 8]               0\n",
            "     BatchNorm2d-108            [-1, 320, 8, 8]             640\n",
            "          Conv2d-109            [-1, 128, 8, 8]          40,960\n",
            "     BatchNorm2d-110            [-1, 128, 8, 8]             256\n",
            "          Conv2d-111             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-112            [-1, 352, 8, 8]               0\n",
            "     BatchNorm2d-113            [-1, 352, 8, 8]             704\n",
            "          Conv2d-114            [-1, 128, 8, 8]          45,056\n",
            "     BatchNorm2d-115            [-1, 128, 8, 8]             256\n",
            "          Conv2d-116             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-117            [-1, 384, 8, 8]               0\n",
            "     BatchNorm2d-118            [-1, 384, 8, 8]             768\n",
            "          Conv2d-119            [-1, 128, 8, 8]          49,152\n",
            "     BatchNorm2d-120            [-1, 128, 8, 8]             256\n",
            "          Conv2d-121             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-122            [-1, 416, 8, 8]               0\n",
            "     BatchNorm2d-123            [-1, 416, 8, 8]             832\n",
            "          Conv2d-124            [-1, 128, 8, 8]          53,248\n",
            "     BatchNorm2d-125            [-1, 128, 8, 8]             256\n",
            "          Conv2d-126             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-127            [-1, 448, 8, 8]               0\n",
            "     BatchNorm2d-128            [-1, 448, 8, 8]             896\n",
            "          Conv2d-129            [-1, 128, 8, 8]          57,344\n",
            "     BatchNorm2d-130            [-1, 128, 8, 8]             256\n",
            "          Conv2d-131             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-132            [-1, 480, 8, 8]               0\n",
            "     BatchNorm2d-133            [-1, 480, 8, 8]             960\n",
            "          Conv2d-134            [-1, 128, 8, 8]          61,440\n",
            "     BatchNorm2d-135            [-1, 128, 8, 8]             256\n",
            "          Conv2d-136             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-137            [-1, 512, 8, 8]               0\n",
            "     BatchNorm2d-138            [-1, 512, 8, 8]           1,024\n",
            "          Conv2d-139            [-1, 128, 8, 8]          65,536\n",
            "     BatchNorm2d-140            [-1, 128, 8, 8]             256\n",
            "          Conv2d-141             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-142            [-1, 544, 8, 8]               0\n",
            "     BatchNorm2d-143            [-1, 544, 8, 8]           1,088\n",
            "          Conv2d-144            [-1, 128, 8, 8]          69,632\n",
            "     BatchNorm2d-145            [-1, 128, 8, 8]             256\n",
            "          Conv2d-146             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-147            [-1, 576, 8, 8]               0\n",
            "     BatchNorm2d-148            [-1, 576, 8, 8]           1,152\n",
            "          Conv2d-149            [-1, 128, 8, 8]          73,728\n",
            "     BatchNorm2d-150            [-1, 128, 8, 8]             256\n",
            "          Conv2d-151             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-152            [-1, 608, 8, 8]               0\n",
            "     BatchNorm2d-153            [-1, 608, 8, 8]           1,216\n",
            "          Conv2d-154            [-1, 128, 8, 8]          77,824\n",
            "     BatchNorm2d-155            [-1, 128, 8, 8]             256\n",
            "          Conv2d-156             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-157            [-1, 640, 8, 8]               0\n",
            "     BatchNorm2d-158            [-1, 640, 8, 8]           1,280\n",
            "          Conv2d-159            [-1, 128, 8, 8]          81,920\n",
            "     BatchNorm2d-160            [-1, 128, 8, 8]             256\n",
            "          Conv2d-161             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-162            [-1, 672, 8, 8]               0\n",
            "     BatchNorm2d-163            [-1, 672, 8, 8]           1,344\n",
            "          Conv2d-164            [-1, 128, 8, 8]          86,016\n",
            "     BatchNorm2d-165            [-1, 128, 8, 8]             256\n",
            "          Conv2d-166             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-167            [-1, 704, 8, 8]               0\n",
            "     BatchNorm2d-168            [-1, 704, 8, 8]           1,408\n",
            "          Conv2d-169            [-1, 128, 8, 8]          90,112\n",
            "     BatchNorm2d-170            [-1, 128, 8, 8]             256\n",
            "          Conv2d-171             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-172            [-1, 736, 8, 8]               0\n",
            "     BatchNorm2d-173            [-1, 736, 8, 8]           1,472\n",
            "          Conv2d-174            [-1, 128, 8, 8]          94,208\n",
            "     BatchNorm2d-175            [-1, 128, 8, 8]             256\n",
            "          Conv2d-176             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-177            [-1, 768, 8, 8]               0\n",
            "     BatchNorm2d-178            [-1, 768, 8, 8]           1,536\n",
            "          Conv2d-179            [-1, 128, 8, 8]          98,304\n",
            "     BatchNorm2d-180            [-1, 128, 8, 8]             256\n",
            "          Conv2d-181             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-182            [-1, 800, 8, 8]               0\n",
            "     BatchNorm2d-183            [-1, 800, 8, 8]           1,600\n",
            "          Conv2d-184            [-1, 128, 8, 8]         102,400\n",
            "     BatchNorm2d-185            [-1, 128, 8, 8]             256\n",
            "          Conv2d-186             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-187            [-1, 832, 8, 8]               0\n",
            "     BatchNorm2d-188            [-1, 832, 8, 8]           1,664\n",
            "          Conv2d-189            [-1, 128, 8, 8]         106,496\n",
            "     BatchNorm2d-190            [-1, 128, 8, 8]             256\n",
            "          Conv2d-191             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-192            [-1, 864, 8, 8]               0\n",
            "     BatchNorm2d-193            [-1, 864, 8, 8]           1,728\n",
            "          Conv2d-194            [-1, 128, 8, 8]         110,592\n",
            "     BatchNorm2d-195            [-1, 128, 8, 8]             256\n",
            "          Conv2d-196             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-197            [-1, 896, 8, 8]               0\n",
            "     BatchNorm2d-198            [-1, 896, 8, 8]           1,792\n",
            "          Conv2d-199            [-1, 128, 8, 8]         114,688\n",
            "     BatchNorm2d-200            [-1, 128, 8, 8]             256\n",
            "          Conv2d-201             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-202            [-1, 928, 8, 8]               0\n",
            "     BatchNorm2d-203            [-1, 928, 8, 8]           1,856\n",
            "          Conv2d-204            [-1, 128, 8, 8]         118,784\n",
            "     BatchNorm2d-205            [-1, 128, 8, 8]             256\n",
            "          Conv2d-206             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-207            [-1, 960, 8, 8]               0\n",
            "     BatchNorm2d-208            [-1, 960, 8, 8]           1,920\n",
            "          Conv2d-209            [-1, 128, 8, 8]         122,880\n",
            "     BatchNorm2d-210            [-1, 128, 8, 8]             256\n",
            "          Conv2d-211             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-212            [-1, 992, 8, 8]               0\n",
            "     BatchNorm2d-213            [-1, 992, 8, 8]           1,984\n",
            "          Conv2d-214            [-1, 128, 8, 8]         126,976\n",
            "     BatchNorm2d-215            [-1, 128, 8, 8]             256\n",
            "          Conv2d-216             [-1, 32, 8, 8]          36,864\n",
            "      Bottleneck-217           [-1, 1024, 8, 8]               0\n",
            "     BatchNorm2d-218           [-1, 1024, 8, 8]           2,048\n",
            "          Conv2d-219            [-1, 512, 8, 8]         524,288\n",
            "      Transition-220            [-1, 512, 4, 4]               0\n",
            "     BatchNorm2d-221            [-1, 512, 4, 4]           1,024\n",
            "          Conv2d-222            [-1, 128, 4, 4]          65,536\n",
            "     BatchNorm2d-223            [-1, 128, 4, 4]             256\n",
            "          Conv2d-224             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-225            [-1, 544, 4, 4]               0\n",
            "     BatchNorm2d-226            [-1, 544, 4, 4]           1,088\n",
            "          Conv2d-227            [-1, 128, 4, 4]          69,632\n",
            "     BatchNorm2d-228            [-1, 128, 4, 4]             256\n",
            "          Conv2d-229             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-230            [-1, 576, 4, 4]               0\n",
            "     BatchNorm2d-231            [-1, 576, 4, 4]           1,152\n",
            "          Conv2d-232            [-1, 128, 4, 4]          73,728\n",
            "     BatchNorm2d-233            [-1, 128, 4, 4]             256\n",
            "          Conv2d-234             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-235            [-1, 608, 4, 4]               0\n",
            "     BatchNorm2d-236            [-1, 608, 4, 4]           1,216\n",
            "          Conv2d-237            [-1, 128, 4, 4]          77,824\n",
            "     BatchNorm2d-238            [-1, 128, 4, 4]             256\n",
            "          Conv2d-239             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-240            [-1, 640, 4, 4]               0\n",
            "     BatchNorm2d-241            [-1, 640, 4, 4]           1,280\n",
            "          Conv2d-242            [-1, 128, 4, 4]          81,920\n",
            "     BatchNorm2d-243            [-1, 128, 4, 4]             256\n",
            "          Conv2d-244             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-245            [-1, 672, 4, 4]               0\n",
            "     BatchNorm2d-246            [-1, 672, 4, 4]           1,344\n",
            "          Conv2d-247            [-1, 128, 4, 4]          86,016\n",
            "     BatchNorm2d-248            [-1, 128, 4, 4]             256\n",
            "          Conv2d-249             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-250            [-1, 704, 4, 4]               0\n",
            "     BatchNorm2d-251            [-1, 704, 4, 4]           1,408\n",
            "          Conv2d-252            [-1, 128, 4, 4]          90,112\n",
            "     BatchNorm2d-253            [-1, 128, 4, 4]             256\n",
            "          Conv2d-254             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-255            [-1, 736, 4, 4]               0\n",
            "     BatchNorm2d-256            [-1, 736, 4, 4]           1,472\n",
            "          Conv2d-257            [-1, 128, 4, 4]          94,208\n",
            "     BatchNorm2d-258            [-1, 128, 4, 4]             256\n",
            "          Conv2d-259             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-260            [-1, 768, 4, 4]               0\n",
            "     BatchNorm2d-261            [-1, 768, 4, 4]           1,536\n",
            "          Conv2d-262            [-1, 128, 4, 4]          98,304\n",
            "     BatchNorm2d-263            [-1, 128, 4, 4]             256\n",
            "          Conv2d-264             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-265            [-1, 800, 4, 4]               0\n",
            "     BatchNorm2d-266            [-1, 800, 4, 4]           1,600\n",
            "          Conv2d-267            [-1, 128, 4, 4]         102,400\n",
            "     BatchNorm2d-268            [-1, 128, 4, 4]             256\n",
            "          Conv2d-269             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-270            [-1, 832, 4, 4]               0\n",
            "     BatchNorm2d-271            [-1, 832, 4, 4]           1,664\n",
            "          Conv2d-272            [-1, 128, 4, 4]         106,496\n",
            "     BatchNorm2d-273            [-1, 128, 4, 4]             256\n",
            "          Conv2d-274             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-275            [-1, 864, 4, 4]               0\n",
            "     BatchNorm2d-276            [-1, 864, 4, 4]           1,728\n",
            "          Conv2d-277            [-1, 128, 4, 4]         110,592\n",
            "     BatchNorm2d-278            [-1, 128, 4, 4]             256\n",
            "          Conv2d-279             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-280            [-1, 896, 4, 4]               0\n",
            "     BatchNorm2d-281            [-1, 896, 4, 4]           1,792\n",
            "          Conv2d-282            [-1, 128, 4, 4]         114,688\n",
            "     BatchNorm2d-283            [-1, 128, 4, 4]             256\n",
            "          Conv2d-284             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-285            [-1, 928, 4, 4]               0\n",
            "     BatchNorm2d-286            [-1, 928, 4, 4]           1,856\n",
            "          Conv2d-287            [-1, 128, 4, 4]         118,784\n",
            "     BatchNorm2d-288            [-1, 128, 4, 4]             256\n",
            "          Conv2d-289             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-290            [-1, 960, 4, 4]               0\n",
            "     BatchNorm2d-291            [-1, 960, 4, 4]           1,920\n",
            "          Conv2d-292            [-1, 128, 4, 4]         122,880\n",
            "     BatchNorm2d-293            [-1, 128, 4, 4]             256\n",
            "          Conv2d-294             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-295            [-1, 992, 4, 4]               0\n",
            "     BatchNorm2d-296            [-1, 992, 4, 4]           1,984\n",
            "          Conv2d-297            [-1, 128, 4, 4]         126,976\n",
            "     BatchNorm2d-298            [-1, 128, 4, 4]             256\n",
            "          Conv2d-299             [-1, 32, 4, 4]          36,864\n",
            "      Bottleneck-300           [-1, 1024, 4, 4]               0\n",
            "     BatchNorm2d-301           [-1, 1024, 4, 4]           2,048\n",
            "          Linear-302                   [-1, 10]          10,250\n",
            "        DenseNet-303                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 6,956,298\n",
            "Trainable params: 6,956,298\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 78.50\n",
            "Params size (MB): 26.54\n",
            "Estimated Total Size (MB): 105.05\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqk-qEJC8n2s"
      },
      "source": [
        "### 학습 (Train) & 검증 (Validation) 함수 정의\r\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvRdPmEb8m2b"
      },
      "source": [
        "def train(epoch, max_epoch):\r\n",
        "    print(f\"Train Epoch [{epoch}/{max_epoch}]\")\r\n",
        "    model.train() # Model to train mode\r\n",
        "\r\n",
        "    train_loss = 0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    acc = 0\r\n",
        "\r\n",
        "    for idx, (x, y) in enumerate(train_dataloader):\r\n",
        "        x = x.to(device) # maps data into GPU memory\r\n",
        "        y = y.to(device)\r\n",
        "\r\n",
        "        optimizer.zero_grad() # reset gradients in optimizer before calculating the loss\r\n",
        "\r\n",
        "        y_pred = model(x) # model inference\r\n",
        "        loss = criterion(y_pred, y) # calculating the loss\r\n",
        "\r\n",
        "        loss.backward() # back-propagation to get cumulative gradients\r\n",
        "\r\n",
        "        optimizer.step() # update model parameters\r\n",
        "        train_loss += loss.item()\r\n",
        "        _, inference = y_pred.max(1)\r\n",
        "\r\n",
        "        total += x.size(0)\r\n",
        "        correct += inference.eq(y).sum().item()\r\n",
        "\r\n",
        "        if idx % 100 == 0:\r\n",
        "            print(f\"Epoch [{epoch}/{max_epoch}] Batch [{idx}] Train Loss: {loss.item()}\")\r\n",
        "\r\n",
        "    acc = 100*correct/total\r\n",
        "    print(f\"Epoch [{epoch}/{max_epoch}] Train Loss: {train_loss/total} Train Accuracy: {acc}\")\r\n",
        "\r\n",
        "def valid(epoch, max_epoch):\r\n",
        "    print(f\"Valid Epoch [{epoch}/{max_epoch}]\")\r\n",
        "    model.eval() # Model to evaluation mode\r\n",
        "\r\n",
        "    valid_loss = 0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "\r\n",
        "    for idx, (x, y) in enumerate(valid_dataloader):\r\n",
        "        x = x.to(device) # maps data into GPU memory\r\n",
        "        y = y.to(device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            y_pred = model(x) # model inference\r\n",
        "            valid_loss += criterion(y_pred, y).item()\r\n",
        "\r\n",
        "            _, inference = y_pred.max(1)\r\n",
        "\r\n",
        "            total += x.size(0)\r\n",
        "            correct += inference.eq(y).sum().item()\r\n",
        "\r\n",
        "    acc = 100*correct/total\r\n",
        "    print(f\"Epoch [{epoch}/{max_epoch}] Valid Loss: {valid_loss/total} Valid Accuracy: {acc}\")\r\n",
        "\r\n",
        "    if not os.path.exists('checkpoint'):\r\n",
        "        os.mkdir('checkpoint')\r\n",
        "\r\n",
        "    torch.save(model.state_dict(), f'checkpoint/{model_path}')\r\n",
        "    print(f\"Epoch [{epoch}/{max_epoch}] Valid Model Saved: checkpoint/{model_path}\")\r\n",
        "\r\n",
        "# Custom leraning rate scheduler is use.\r\n",
        "# Decay learning rate by 10 at epoch 5.\r\n",
        "def lr_schedule(optimizer, epoch):\r\n",
        "    if epoch == 5:\r\n",
        "        lr = learning_rate / 10\r\n",
        "        for param_group in optimizer.param_groups:\r\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0_ii30R0Ci7"
      },
      "source": [
        "### **Data Preparation**\r\n",
        "* In this **Tutorial**, we are going to use **torchvision** which contains famous vision dataset, and we will use **CIFAR-10** especially \r\n",
        "* **CIFAR-10** is dataset for classifying image into **10** different categories.\r\n",
        "* The **10** different classes represent **airplanes**, **cars**, **birds**, **cats**, **deer**, **dogs**, **frogs**, **horses**, **ships**, and **trucks**. \r\n",
        "* There are **6,000** images of each class.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "cf1e1e94e8d24f67b1c575f385807daf",
            "7d16064f562740d4ba9038eadb2e7be4",
            "ba7db11c30514a25b60fae45d92dacb6",
            "3b744ece9d9e4d50ac69ca2e725a2c17",
            "9cdf334e8b684beca54d1cd6247f39fc",
            "64282ca47dd04bebba0d1726d0384e94",
            "6a2cf6538a10439fa452700e65384077",
            "38b550cffbec4232943597c6ccdfafcb"
          ]
        },
        "id": "onYJiudv0Cog",
        "outputId": "bb4953ad-e605-4280-b418-3cc5f1b9a702"
      },
      "source": [
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "transform = transforms.Compose([\r\n",
        "  transforms.ToTensor(),\r\n",
        "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r\n",
        "])\r\n",
        "\r\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\r\n",
        "valid_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\r\n",
        "\r\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\r\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf1e1e94e8d24f67b1c575f385807daf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfFq6lHyc9ss"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-2cOzGS9NvE",
        "outputId": "d6ac8687-7567-4364-9c2b-da4edabfd347"
      },
      "source": [
        "for epoch in range(0, max_epoch):\r\n",
        "  lr_schedule(optimizer, epoch)\r\n",
        "  train(epoch, max_epoch)\r\n",
        "  valid(epoch, max_epoch)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch [0/10]\n",
            "Epoch [0/10] Batch [0] Train Loss: 2.2806129455566406\n",
            "Epoch [0/10] Batch [100] Train Loss: 1.8704365491867065\n",
            "Epoch [0/10] Batch [200] Train Loss: 1.5759059190750122\n",
            "Epoch [0/10] Batch [300] Train Loss: 1.4987863302230835\n",
            "Epoch [0/10] Train Loss: 0.012912284767627716 Train Accuracy: 39.414\n",
            "Valid Epoch [0/10]\n",
            "Epoch [0/10] Valid Loss: 0.010671390163898469 Valid Accuracy: 50.39\n",
            "Epoch [0/10] Valid Model Saved: checkpoint/densenet121_cifar10.pt\n",
            "Train Epoch [1/10]\n",
            "Epoch [1/10] Batch [0] Train Loss: 1.4521450996398926\n",
            "Epoch [1/10] Batch [100] Train Loss: 1.0664825439453125\n",
            "Epoch [1/10] Batch [200] Train Loss: 1.0352414846420288\n",
            "Epoch [1/10] Batch [300] Train Loss: 0.9708278775215149\n",
            "Epoch [1/10] Train Loss: 0.008630217782258988 Train Accuracy: 60.538\n",
            "Valid Epoch [1/10]\n",
            "Epoch [1/10] Valid Loss: 0.008917009031772614 Valid Accuracy: 60.44\n",
            "Epoch [1/10] Valid Model Saved: checkpoint/densenet121_cifar10.pt\n",
            "Train Epoch [2/10]\n",
            "Epoch [2/10] Batch [0] Train Loss: 0.8301960825920105\n",
            "Epoch [2/10] Batch [100] Train Loss: 0.7258948683738708\n",
            "Epoch [2/10] Batch [200] Train Loss: 0.9635822772979736\n",
            "Epoch [2/10] Batch [300] Train Loss: 0.6801140904426575\n",
            "Epoch [2/10] Train Loss: 0.006492732907533646 Train Accuracy: 70.538\n",
            "Valid Epoch [2/10]\n",
            "Epoch [2/10] Valid Loss: 0.00845401880145073 Valid Accuracy: 67.32\n",
            "Epoch [2/10] Valid Model Saved: checkpoint/densenet121_cifar10.pt\n",
            "Train Epoch [3/10]\n",
            "Epoch [3/10] Batch [0] Train Loss: 0.8033885955810547\n",
            "Epoch [3/10] Batch [100] Train Loss: 0.7143118381500244\n",
            "Epoch [3/10] Batch [200] Train Loss: 0.6789661049842834\n",
            "Epoch [3/10] Batch [300] Train Loss: 0.6356138586997986\n",
            "Epoch [3/10] Train Loss: 0.005354904910326004 Train Accuracy: 75.706\n",
            "Valid Epoch [3/10]\n",
            "Epoch [3/10] Valid Loss: 0.005991931176185608 Valid Accuracy: 74.12\n",
            "Epoch [3/10] Valid Model Saved: checkpoint/densenet121_cifar10.pt\n",
            "Train Epoch [4/10]\n",
            "Epoch [4/10] Batch [0] Train Loss: 0.6038087606430054\n",
            "Epoch [4/10] Batch [100] Train Loss: 0.740476131439209\n",
            "Epoch [4/10] Batch [200] Train Loss: 0.5783843994140625\n",
            "Epoch [4/10] Batch [300] Train Loss: 0.5016093850135803\n",
            "Epoch [4/10] Train Loss: 0.004400768575072289 Train Accuracy: 80.342\n",
            "Valid Epoch [4/10]\n",
            "Epoch [4/10] Valid Loss: 0.0052365755200386045 Valid Accuracy: 76.67\n",
            "Epoch [4/10] Valid Model Saved: checkpoint/densenet121_cifar10.pt\n",
            "Train Epoch [5/10]\n",
            "Epoch [5/10] Batch [0] Train Loss: 0.5142467617988586\n",
            "Epoch [5/10] Batch [100] Train Loss: 0.36507031321525574\n",
            "Epoch [5/10] Batch [200] Train Loss: 0.31812262535095215\n",
            "Epoch [5/10] Batch [300] Train Loss: 0.2673238515853882\n",
            "Epoch [5/10] Train Loss: 0.0027834440273046494 Train Accuracy: 87.71\n",
            "Valid Epoch [5/10]\n",
            "Epoch [5/10] Valid Loss: 0.0037262821286916732 Valid Accuracy: 84.03\n",
            "Epoch [5/10] Valid Model Saved: checkpoint/densenet121_cifar10.pt\n",
            "Train Epoch [6/10]\n",
            "Epoch [6/10] Batch [0] Train Loss: 0.20728126168251038\n",
            "Epoch [6/10] Batch [100] Train Loss: 0.31557393074035645\n",
            "Epoch [6/10] Batch [200] Train Loss: 0.1823340505361557\n",
            "Epoch [6/10] Batch [300] Train Loss: 0.21425077319145203\n",
            "Epoch [6/10] Train Loss: 0.002261979186832905 Train Accuracy: 89.922\n",
            "Valid Epoch [6/10]\n",
            "Epoch [6/10] Valid Loss: 0.003694413211941719 Valid Accuracy: 83.92\n",
            "Epoch [6/10] Valid Model Saved: checkpoint/densenet121_cifar10.pt\n",
            "Train Epoch [7/10]\n",
            "Epoch [7/10] Batch [0] Train Loss: 0.27066895365715027\n",
            "Epoch [7/10] Batch [100] Train Loss: 0.25939902663230896\n",
            "Epoch [7/10] Batch [200] Train Loss: 0.1564444899559021\n",
            "Epoch [7/10] Batch [300] Train Loss: 0.21337024867534637\n",
            "Epoch [7/10] Train Loss: 0.001903797614276409 Train Accuracy: 91.538\n",
            "Valid Epoch [7/10]\n",
            "Epoch [7/10] Valid Loss: 0.0037577819645404817 Valid Accuracy: 84.36\n",
            "Epoch [7/10] Valid Model Saved: checkpoint/densenet121_cifar10.pt\n",
            "Train Epoch [8/10]\n",
            "Epoch [8/10] Batch [0] Train Loss: 0.24434234201908112\n",
            "Epoch [8/10] Batch [100] Train Loss: 0.21057993173599243\n",
            "Epoch [8/10] Batch [200] Train Loss: 0.18715663254261017\n",
            "Epoch [8/10] Batch [300] Train Loss: 0.20142120122909546\n",
            "Epoch [8/10] Train Loss: 0.0015726818364858628 Train Accuracy: 93.166\n",
            "Valid Epoch [8/10]\n",
            "Epoch [8/10] Valid Loss: 0.003798044593632221 Valid Accuracy: 84.7\n",
            "Epoch [8/10] Valid Model Saved: checkpoint/densenet121_cifar10.pt\n",
            "Train Epoch [9/10]\n",
            "Epoch [9/10] Batch [0] Train Loss: 0.1090482696890831\n",
            "Epoch [9/10] Batch [100] Train Loss: 0.07732997089624405\n",
            "Epoch [9/10] Batch [200] Train Loss: 0.22455042600631714\n",
            "Epoch [9/10] Batch [300] Train Loss: 0.14130622148513794\n",
            "Epoch [9/10] Train Loss: 0.0012506432421505451 Train Accuracy: 94.624\n",
            "Valid Epoch [9/10]\n",
            "Epoch [9/10] Valid Loss: 0.004063036102056503 Valid Accuracy: 84.93\n",
            "Epoch [9/10] Valid Model Saved: checkpoint/densenet121_cifar10.pt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}