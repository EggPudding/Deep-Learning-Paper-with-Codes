{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGGNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpBBF1xj6B0dR/ebr6mgHc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EggPudding/Deep-Learning-Practice-with-Codes/blob/main/VGGNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLZXIIeaPeYM"
      },
      "source": [
        "## **Very Deep Convolutional Networks for Large-Scale Image Recognition (ICLR 2015) Tutorial**\r\n",
        "\r\n",
        "*   Pratice for VGGNet Architecture\r\n",
        "*   Orginal Paper: https://arxiv.org/abs/1409.1556\r\n",
        "*   Note that you first change **Runtime** to **GPU** setting\r\n",
        "*   **CIFAR-10** Dataset is used for practice for simplicity\r\n",
        "*   Part of codes from https://github.com/kuangliu/pytorch-cifar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slbq1rU2-kgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb59bad1-90fa-4bd6-cffb-8dad7301330f"
      },
      "source": [
        "!nvidia-smi # Make Sure you are using GPU"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jan 25 02:05:02 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8    10W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2ZRPOuKz8qq"
      },
      "source": [
        "### **VGGNet Model Definition**\r\n",
        "* In this **Tutorial**, Model object for **CIFAR-10** Dataset. \r\n",
        "* **CIFAR-10** is dataset for classifying image into **10** different categories.\r\n",
        "* The **10** different classes represent **airplanes**, **cars**, **birds**, **cats**, **deer**, **dogs**, **frogs**, **horses**, **ships**, and **trucks**. \r\n",
        "* There are **6,000** images of each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UMQ5anWvUu1"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "import torch.backends.cudnn as cudnn\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "import os\r\n",
        "\r\n",
        "# VGGNet contains various depth models such as VGG-11, VGG-18.\r\n",
        "# But in this tutorial VGG-11 is utilized since we use simple MNIST Dataset.\r\n",
        "# Note that all Conv Layers has kernel_size 3.\r\n",
        "class VGGNet(nn.Module):\r\n",
        "    def __init__(self, num_classes=10, init_weight=True):\r\n",
        "        super(VGGNet, self).__init__()\r\n",
        "        self.feature_extract = nn.Sequential(\r\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\r\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\r\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\r\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\r\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\r\n",
        "        )\r\n",
        "\r\n",
        "        self.classifier = nn.Sequential(\r\n",
        "            nn.Dropout(),\r\n",
        "            nn.Linear(512, 512),\r\n",
        "            nn.ReLU(True),\r\n",
        "            nn.Dropout(),\r\n",
        "            nn.Linear(512, 512),\r\n",
        "            nn.ReLU(True),\r\n",
        "            nn.Linear(512, 10),       \r\n",
        "        )\r\n",
        "\r\n",
        "        if init_weight:\r\n",
        "            self._initialize_weights()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.feature_extract(x)\r\n",
        "        x = torch.flatten(x, 1)\r\n",
        "        x = self.classifier(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "    # Weight Initialization\r\n",
        "    def _initialize_weights(self):\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n",
        "                if m.bias is not None:\r\n",
        "                    nn.init.constant_(m.bias, 0)\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                nn.init.constant_(m.weight, 1)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "            elif isinstance(m, nn.Linear):\r\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\r\n",
        "                nn.init.constant_(m.bias, 0)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2p5PaQ-8B3v"
      },
      "source": [
        "### **Hyper Parameter Setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMnPH0YG8FuW"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # whether using gpu or cpu\r\n",
        "\r\n",
        "model = VGGNet() # model assignment\r\n",
        "model.to(device) # mapping model weight & bias into gpu memory\r\n",
        "model = torch.nn.DataParallel(model) # used for parallel setting\r\n",
        "\r\n",
        "cudnn.benchmark = True # using cudnn which optimizes the algorithm\r\n",
        "\r\n",
        "learning_rate = 0.01\r\n",
        "batch_size = 128\r\n",
        "max_epoch = 10\r\n",
        "\r\n",
        "model_path = 'vggnet_cifar10.pt'\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss() # simple cross-entropy is utilized\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimizer utilized"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EokrFaIYRQdS"
      },
      "source": [
        "* **torchsummary** is package for visualizing pytorch model.\r\n",
        "* Layers, Number of parameters can be viewed through this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AefbsD-30AJM",
        "outputId": "efc6e45a-bcd3-4e7a-d1a6-5d0b6a2c24d8"
      },
      "source": [
        "import torchsummary\r\n",
        "\r\n",
        "torchsummary.summary(model.cuda(), (3, 32, 32)) # CIFAR-10 Image has 32 x 32 x 3 shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "              ReLU-2           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-3           [-1, 64, 16, 16]               0\n",
            "            Conv2d-4          [-1, 128, 16, 16]          73,856\n",
            "         MaxPool2d-5            [-1, 128, 8, 8]               0\n",
            "            Conv2d-6            [-1, 256, 8, 8]         295,168\n",
            "            Conv2d-7            [-1, 256, 8, 8]         590,080\n",
            "         MaxPool2d-8            [-1, 256, 4, 4]               0\n",
            "            Conv2d-9            [-1, 512, 4, 4]       1,180,160\n",
            "           Conv2d-10            [-1, 512, 4, 4]       2,359,808\n",
            "        MaxPool2d-11            [-1, 512, 2, 2]               0\n",
            "           Conv2d-12            [-1, 512, 2, 2]       2,359,808\n",
            "           Conv2d-13            [-1, 512, 2, 2]       2,359,808\n",
            "        MaxPool2d-14            [-1, 512, 1, 1]               0\n",
            "          Dropout-15                  [-1, 512]               0\n",
            "           Linear-16                  [-1, 512]         262,656\n",
            "             ReLU-17                  [-1, 512]               0\n",
            "          Dropout-18                  [-1, 512]               0\n",
            "           Linear-19                  [-1, 512]         262,656\n",
            "             ReLU-20                  [-1, 512]               0\n",
            "           Linear-21                   [-1, 10]           5,130\n",
            "           VGGNet-22                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 9,750,922\n",
            "Trainable params: 9,750,922\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.92\n",
            "Params size (MB): 37.20\n",
            "Estimated Total Size (MB): 39.13\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqk-qEJC8n2s"
      },
      "source": [
        "### **Training and Evaluation function definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvRdPmEb8m2b"
      },
      "source": [
        "def train(epoch, max_epoch):\r\n",
        "    print(f\"Train Epoch [{epoch}/{max_epoch}]\")\r\n",
        "    model.train() # Model to train mode\r\n",
        "\r\n",
        "    train_loss = 0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    acc = 0\r\n",
        "\r\n",
        "    for idx, (x, y) in enumerate(train_dataloader):\r\n",
        "        x = x.to(device) # maps data into GPU memory\r\n",
        "        y = y.to(device)\r\n",
        "\r\n",
        "        optimizer.zero_grad() # reset gradients in optimizer before calculating the loss\r\n",
        "\r\n",
        "        y_pred = model(x) # model inference\r\n",
        "        loss = criterion(y_pred, y) # calculating the loss\r\n",
        "\r\n",
        "        loss.backward() # back-propagation to get cumulative gradients\r\n",
        "\r\n",
        "        optimizer.step() # update model parameters\r\n",
        "        train_loss += loss.item()\r\n",
        "        _, inference = y_pred.max(1)\r\n",
        "\r\n",
        "        total += x.size(0)\r\n",
        "        correct += inference.eq(y).sum().item()\r\n",
        "\r\n",
        "        if idx % 100 == 0:\r\n",
        "            print(f\"Epoch [{epoch}/{max_epoch}] Batch [{idx}] Train Loss: {loss.item()}\")\r\n",
        "\r\n",
        "    acc = 100*correct/total\r\n",
        "    print(f\"Epoch [{epoch}/{max_epoch}] Train Loss: {train_loss/total} Train Accuracy: {acc}\")\r\n",
        "\r\n",
        "def valid(epoch, max_epoch):\r\n",
        "    print(f\"Valid Epoch [{epoch}/{max_epoch}]\")\r\n",
        "    model.eval() # Model to evaluation mode\r\n",
        "\r\n",
        "    valid_loss = 0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "\r\n",
        "    for idx, (x, y) in enumerate(valid_dataloader):\r\n",
        "        x = x.to(device) # maps data into GPU memory\r\n",
        "        y = y.to(device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            y_pred = model(x) # model inference\r\n",
        "            valid_loss += criterion(y_pred, y).item()\r\n",
        "\r\n",
        "            _, inference = y_pred.max(1)\r\n",
        "\r\n",
        "            total += x.size(0)\r\n",
        "            correct += inference.eq(y).sum().item()\r\n",
        "\r\n",
        "    acc = 100*correct/total\r\n",
        "    print(f\"Epoch [{epoch}/{max_epoch}] Valid Loss: {valid_loss/total} Valid Accuracy: {acc}\")\r\n",
        "\r\n",
        "    if not os.path.exists('checkpoint'):\r\n",
        "        os.mkdir('checkpoint')\r\n",
        "\r\n",
        "    torch.save(model.state_dict(), f'checkpoint/{model_path}')\r\n",
        "    print(f\"Epoch [{epoch}/{max_epoch}] Valid Model Saved: checkpoint/{model_path}\")\r\n",
        "\r\n",
        "# Custom leraning rate scheduler is use.\r\n",
        "# Decay learning rate by 10 at epoch 5.\r\n",
        "def lr_schedule(optimizer, epoch):\r\n",
        "    if epoch == 5:\r\n",
        "        lr = learning_rate / 10\r\n",
        "        for param_group in optimizer.param_groups:\r\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0_ii30R0Ci7"
      },
      "source": [
        "### **Data Preparation**\r\n",
        "* In this **Tutorial**, we are going to use **torchvision** which contains famous vision dataset, and we will use **CIFAR-10** especially \r\n",
        "* **CIFAR-10** is dataset for classifying image into **10** different categories.\r\n",
        "* The **10** different classes represent **airplanes**, **cars**, **birds**, **cats**, **deer**, **dogs**, **frogs**, **horses**, **ships**, and **trucks**. \r\n",
        "* There are **6,000** images of each class.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onYJiudv0Cog",
        "outputId": "19fcd2f8-7982-4506-e643-1ed823637077"
      },
      "source": [
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "transform = transforms.Compose([\r\n",
        "  transforms.ToTensor(),\r\n",
        "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r\n",
        "])\r\n",
        "\r\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\r\n",
        "valid_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\r\n",
        "\r\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\r\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50wNmHjCY-FW"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-2cOzGS9NvE",
        "outputId": "0ee8cbca-83a6-4217-cff4-e0abf53894d0"
      },
      "source": [
        "for epoch in range(0, max_epoch):\r\n",
        "    lr_schedule(optimizer, epoch)\r\n",
        "    train(epoch, max_epoch)\r\n",
        "    valid(epoch, max_epoch)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch [0/10]\n",
            "Epoch [0/10] Batch [0] Train Loss: 2.302567958831787\n",
            "Epoch [0/10] Batch [100] Train Loss: 2.302792549133301\n",
            "Epoch [0/10] Batch [200] Train Loss: 2.30121111869812\n",
            "Epoch [0/10] Batch [300] Train Loss: 2.2992587089538574\n",
            "Epoch [0/10] Train Loss: 10294622.855603388 Train Accuracy: 9.984\n",
            "Valid Epoch [0/10]\n",
            "Epoch [0/10] Valid Loss: 0.018192924785614014 Valid Accuracy: 10.0\n",
            "Epoch [0/10] Valid Model Saved: checkpoint/vggnet_cifar10.pt\n",
            "Train Epoch [1/10]\n",
            "Epoch [1/10] Batch [0] Train Loss: 2.3001837730407715\n",
            "Epoch [1/10] Batch [100] Train Loss: 2.304408311843872\n",
            "Epoch [1/10] Batch [200] Train Loss: 2.305873155593872\n",
            "Epoch [1/10] Batch [300] Train Loss: 4775998390272.0\n",
            "Epoch [1/10] Train Loss: 20976610627.678074 Train Accuracy: 9.996\n",
            "Valid Epoch [1/10]\n",
            "Epoch [1/10] Valid Loss: 0.01819319784641266 Valid Accuracy: 10.0\n",
            "Epoch [1/10] Valid Model Saved: checkpoint/vggnet_cifar10.pt\n",
            "Train Epoch [2/10]\n",
            "Epoch [2/10] Batch [0] Train Loss: 2.301635980606079\n",
            "Epoch [2/10] Batch [100] Train Loss: 2.306386947631836\n",
            "Epoch [2/10] Batch [200] Train Loss: 2.298731565475464\n",
            "Epoch [2/10] Batch [300] Train Loss: 2.305776357650757\n",
            "Epoch [2/10] Train Loss: 362624424.3635659 Train Accuracy: 9.914\n",
            "Valid Epoch [2/10]\n",
            "Epoch [2/10] Valid Loss: 0.018193912649154664 Valid Accuracy: 10.0\n",
            "Epoch [2/10] Valid Model Saved: checkpoint/vggnet_cifar10.pt\n",
            "Train Epoch [3/10]\n",
            "Epoch [3/10] Batch [0] Train Loss: 2.3029911518096924\n",
            "Epoch [3/10] Batch [100] Train Loss: 2.3025879859924316\n",
            "Epoch [3/10] Batch [200] Train Loss: 2.3097519874572754\n",
            "Epoch [3/10] Batch [300] Train Loss: 2.3017778396606445\n",
            "Epoch [3/10] Train Loss: 0.018011453189849855 Train Accuracy: 9.932\n",
            "Valid Epoch [3/10]\n",
            "Epoch [3/10] Valid Loss: 0.0181949036359787 Valid Accuracy: 10.0\n",
            "Epoch [3/10] Valid Model Saved: checkpoint/vggnet_cifar10.pt\n",
            "Train Epoch [4/10]\n",
            "Epoch [4/10] Batch [0] Train Loss: 2.299875020980835\n",
            "Epoch [4/10] Batch [100] Train Loss: 2.302609443664551\n",
            "Epoch [4/10] Batch [200] Train Loss: 2.304447650909424\n",
            "Epoch [4/10] Batch [300] Train Loss: 2.299140453338623\n",
            "Epoch [4/10] Train Loss: 0.018012779712677 Train Accuracy: 9.84\n",
            "Valid Epoch [4/10]\n",
            "Epoch [4/10] Valid Loss: 0.01819507155418396 Valid Accuracy: 10.0\n",
            "Epoch [4/10] Valid Model Saved: checkpoint/vggnet_cifar10.pt\n",
            "Train Epoch [5/10]\n",
            "Epoch [5/10] Batch [0] Train Loss: 2.3000264167785645\n",
            "Epoch [5/10] Batch [100] Train Loss: 2.303407907485962\n",
            "Epoch [5/10] Batch [200] Train Loss: 2.302532911300659\n",
            "Epoch [5/10] Batch [300] Train Loss: 2.3009490966796875\n",
            "Epoch [5/10] Train Loss: 0.018008270230293274 Train Accuracy: 9.852\n",
            "Valid Epoch [5/10]\n",
            "Epoch [5/10] Valid Loss: 0.018190741872787474 Valid Accuracy: 10.0\n",
            "Epoch [5/10] Valid Model Saved: checkpoint/vggnet_cifar10.pt\n",
            "Train Epoch [6/10]\n",
            "Epoch [6/10] Batch [0] Train Loss: 2.30202054977417\n",
            "Epoch [6/10] Batch [100] Train Loss: 2.3024725914001465\n",
            "Epoch [6/10] Batch [200] Train Loss: 2.3019700050354004\n",
            "Epoch [6/10] Batch [300] Train Loss: 2.302065849304199\n",
            "Epoch [6/10] Train Loss: 0.018007050848007203 Train Accuracy: 9.81\n",
            "Valid Epoch [6/10]\n",
            "Epoch [6/10] Valid Loss: 0.01819059600830078 Valid Accuracy: 10.0\n",
            "Epoch [6/10] Valid Model Saved: checkpoint/vggnet_cifar10.pt\n",
            "Train Epoch [7/10]\n",
            "Epoch [7/10] Batch [0] Train Loss: 2.3021278381347656\n",
            "Epoch [7/10] Batch [100] Train Loss: 2.303133010864258\n",
            "Epoch [7/10] Batch [200] Train Loss: 2.3027727603912354\n",
            "Epoch [7/10] Batch [300] Train Loss: 2.30184268951416\n",
            "Epoch [7/10] Train Loss: 0.018007114157676696 Train Accuracy: 9.872\n",
            "Valid Epoch [7/10]\n",
            "Epoch [7/10] Valid Loss: 0.018190400052070618 Valid Accuracy: 10.0\n",
            "Epoch [7/10] Valid Model Saved: checkpoint/vggnet_cifar10.pt\n",
            "Train Epoch [8/10]\n",
            "Epoch [8/10] Batch [0] Train Loss: 2.303083896636963\n",
            "Epoch [8/10] Batch [100] Train Loss: 2.3022773265838623\n",
            "Epoch [8/10] Batch [200] Train Loss: 2.3021953105926514\n",
            "Epoch [8/10] Batch [300] Train Loss: 2.302734851837158\n",
            "Epoch [8/10] Train Loss: 0.018007136535644532 Train Accuracy: 9.712\n",
            "Valid Epoch [8/10]\n",
            "Epoch [8/10] Valid Loss: 0.0181905962228775 Valid Accuracy: 10.0\n",
            "Epoch [8/10] Valid Model Saved: checkpoint/vggnet_cifar10.pt\n",
            "Train Epoch [9/10]\n",
            "Epoch [9/10] Batch [0] Train Loss: 2.3027851581573486\n",
            "Epoch [9/10] Batch [100] Train Loss: 2.3031134605407715\n",
            "Epoch [9/10] Batch [200] Train Loss: 2.3031232357025146\n",
            "Epoch [9/10] Batch [300] Train Loss: 2.3031256198883057\n",
            "Epoch [9/10] Train Loss: 0.01800706073284149 Train Accuracy: 9.834\n",
            "Valid Epoch [9/10]\n",
            "Epoch [9/10] Valid Loss: 0.01819059190750122 Valid Accuracy: 10.0\n",
            "Epoch [9/10] Valid Model Saved: checkpoint/vggnet_cifar10.pt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}